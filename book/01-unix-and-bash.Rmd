# Using UNIX-like computers and mastering the command shell {#unix-and-shell}

Most programs in bioinformatics are written for UNIX-like operating systems like Linux. Moreover,
they are usually controlled from the command line (CLI) and not from a graphical interface (GUI).

UNIX and it's derivatives have a long history as an operating system by programmers for programmers.
This makes UNIX different from commercial operating systems and there is a clear philosophy behind
many aspects of behaviour of a computer running UNIX. That being said, a modern UNIX-like operating
system like Linux is in many ways similar to e.g. Apple's OS X or Windows and is fully controllable
via a graphical interface. However, behind the graphical interface the tools many long-time users
have come to love still remain. Apple's OS X is in this way similar to e.g. Linux by being a UNIX at
its core, with a proprietary graphical interface although in this case a commercial, non-free
program. 

## Interacting with UNIX-like operating systems from commercial operating systems {#interacting-with-unix}

In scientific computing, the most common way to connect to a computing cluster running Linux, is to
use Secure Shell (ssh). 

### Connecting from a UNIX-like computer (Linux or Mac OS X) {#connecting-from-unix}

SSH is a command-line tool available in all Linux distributions and in OS X. You connect like this:

```
ssh user@rackham.uppmax.uu.se
```

And your local command-line session is exchanged for one at the cluster.

### Connecting from a Windows computer {#connecting-from-windows}

It's a little bit more complicated with Windows computers, because there's no ssh program directly
available. There are some options though. At UPPMAX they recommend the
[MobaXterm](http://mobaxterm.mobatek.net/) program (see [UPPMAX
documentation](http://www.uppmax.uu.se/support/user-guides/guide--first-login-to-uppmax/)). Another
popular program is [PuTTY](https://putty.org/).

There are also bash implementations for Windows, e.g. [Cygwin](https://www.cygwin.com/) and the
command line interface that comes with [Git for Windows](https://git-scm.com/download/win).

(I also came across a [blog post describing Windows 10's inbuilt Bash
shell](https://www.howtogeek.com/249966/how-to-install-and-use-the-linux-bash-shell-on-windows-10/).)

### Public key authentication {#public-key-authentication}

## Using the Linux command line {#linux-cli}

The Linux command line is a program called the *shell*, and exists in different versions. The most
popular is probably `bash` (short for *B*ourne *a*gain *sh*ell) a derivative of the original *Bourne
shell* `sh`. Together with *Korn shell*, `sh` and `bash` forms a family with similar syntax. A
second family started with *C shell* `csh`, which aimed at being more similar in syntax to the *C*
programming language. In this book, we're going to focus solely on `bash`.

The shell is both a way of interacting interactively with the computer and a programming language, a
*scripting* language. When you use the shell interactively you execute commands and programs and a
shell script is nothing else than a set of commands saved for later, though often generalized with
variables etc. to be more useful.

There are many free sources of information available, one of them the classic [Bash Guide for
Beginners](http://www.tldp.org/LDP/Bash-Beginners-Guide/html/) [@garrels_bash_2008] from "The
Linux Documentation Project" (tldp).

### Getting help {#cli-getting-help}

The classical way of program authors to provide help to users is via manual ("man") pages, readable
with the `man` command. To read the man page for e.g. the find command you:

```
man find
```

The man command uses the `more` command to show the content page by page. Step through pages with
the space key, search with `/` followed by a string and exit by pressing `q`.

Today it's more common to integrate help in the program and make it available with a flag, usually
`--help` or `-v`. In many cases, you get more than one page of output, so you might want to pipe the
output to `more` to read page by page.

```
wget --help|more
```

For complex programs doing many different things it has become increasingly popular to modularise
program invocation and help. A typical example is `git`, which has one overall help plus individual
help for each type of thing you can do with the program.

```
git --help
git clone --help
```

In this particular example, you're actually taken to the manual page for `git clone`, in the latter
of the two commands.

### The environment {#bash-environment}

Your *environment* is a rather abstract concept. One could say that it encompasses all the settings
you have made to make work effective. When you start computing in a UNIX environment, you're likely
to just accept whatever default settings are there, but with time you might become more and more
picky about settings and will start to change them and, when logging in to a new computer, you will
try to as soon as possible get that set the way you want it.

Your environment is configured through various files, some of a general character, others specific
for a program or a set of programs. Many of these files have names that start with a `.` which, in
most cases, make them *hidden* so they do not show up e.g. in normal directory listings. The purpose
of hiding files is to make it a little bit more difficult to change things that might have negative
effects if not done properly.

#### Variables {#bash-variables}

One key part of configuration is to set variables, in particular *environment variables*, that are
read by different programs and affect how they work. Variables are set with an equal sign:
`var=something`, sets the variable "var" to contain the text "something". The value is retrieved by
putting a dollar before the variable name: `$var`, see examples below.

The difference between normal variables and *environment* variables is that the latter are more
visible to other programs. This can be illustrated by running the following commands, which also
shows the syntax of setting variables at the Bash command prompt and checking their contents:

```
# Set a variable
var='Hello world'
# A variable is visible in the current shell:
echo $var
# But not in a subshell
bash    # Starts a subshell
echo $var
exit    # Exits the subshell 
echo $var

# If you export a variable, it becomes an environment variable and is visible in subshells:
export var
bash    # Starts a subshell
echo $var
exit    # Exits the subshell
echo $var
```

Since a subshell is started every time you run a program, variables need to be exported if they are
to be visible by the program. It is hence only *environment* variables that are visible to programs
you start from the command line.

A classical configuration file consists of a number of definitions of environment variables (plus
in some cases other kinds of mechanisms) and can be seen as an executable text file, a script. The
problem with this is that a subshell is started for each new program run, and variables will be set
in the subshell but *not in the current shell*. To run a configuration script in the current shell
instead of in a subshell, you need to *source* it by preceding it with `source`. An example
illustrates this:

```
# Create a small config file
echo "export var='New value for var'" > /tmp/my_config
# Run without sourcing
/bin/bash /tmp/my_config
# Now check what var is set to
echo $var

# Source the file and see what var is set to after
source /tmp/my_config
echo $var
```

Perhaps you noted in the example above that I used both single quotes `'` and double quotes `"`.
They have almost the same meaning. Both types quotes text, the difference is that within double
quotes, variables are extrapolated, but they are not in single quotes:

```
var='Hello world'
echo "$var"     # Types out Hello world
echo '$var'     # Types out $var
```

#### Configuration files {#bash-config-files}

There are two main configuration files for Bash: `.bash_profile` and `.bashrc`. The former is
sourced when you log in whereas the latter is sourced each time a subshell is started. This means
that `.bashrc` is sourced for every program you run since programs are run in subshells.

Of particular interest is the `PATH` environment variable. This contains a list of directories,
separated by `:` characters, in which Bash will search for programs. When you call e.g. `grep`, Bash
will look through the list of directories in your `PATH` for an executable file called "grep" and
run that for you.

Since environment variables are visible in subshells, they need only be set when you log in and are
hence best defined in `.bash_profile`. Other configurations are better placed in `.bashrc`. An
example is `alias` definitions. Today, it is quite common that alias definitions are placed in a
file of their own `.bash_aliases` which is sourced in `.bashrc`.

When you first log on to a UNIX machine it is common that `.bash_profile` and `.bashrc` already
exist, with some default content.

### Pipelines {#pipelines}

The word "pipeline" can mean slightly different things in computing. The most original meaning comes
from the UNIX shell, where it means that you literaly connect the output of one command as input to
the next in a chain^[When the word "pipeline" is used in computational biology, it's usually used
symbolicaly and means that in order to produce a certain output you need to run several programs in
succession. A better term for this is *workflow*.]. 

To truly understand what's going on, you need to know about the three different channels that each
UNIX command has: standard input (STDIN), standard output (STDOUT) and standard error (STDERR). By
default, STDIN is connected to your keyboard and the other two to your screen. The difference
between STDOUT and STDERR is that the latter is reserved for error messages from a program, whereas
the former is used for expected output. The channels can be redirected to files which is done with
`<` for STDIN, `>` for STDOUT and `2>` for STDERR. In principle, a command hence looks like this:

```
cat < input_file > output_file 2> error_message_file
```

The above reads `input_file` and sends that to `output_file`; if an error occurs, messages will be
written to `error_message_file`. (The example is a bit constructed, because if you don't redirect
STDIN, `cat` actually reads one or more files on the command line, e.g.: `cat file1 file2 file3 >
concatenated_file`.)

The pipe character `|` connects STDOUT from one command, to STDIN for the next, which makes it
possible to modify the output of one command and form a *pipeline*. A very common example is to
place `more` as the last output of a command that produces a lot of output, so you can read the
output line by line.

```
# Find all files ending with `.tsv` and check output page by page with `more`.
find . -name "*.tsv" | more
```

Pipelines can be arbitrarily long and often contain programs like `grep` (scans for text patterns),
`sed` or `awk` (modifies text), `tail -f` (follow output when it's being produced). This is in line
with the *UNIX philosophy* that programs should do one thing and do it well, leaving other things
for other programs.

### Job control {#job-control}

`&`, `bg`, `fg`, `jobs` and &lt;ctrl&gt;-Z.

## Users, files, directories and file systems {#users-files-and-filesystems}

### Users, groups and permissions {#users-groups-permissions}

A *user* in a UNIX system is what you log in as. Any files you create when you're working in the
computer will be *owned* by this user plus a *group*. Whereas you can only be one user (unless you
have multiple accounts), each user can belong to more than one group. The group that owns a file
will be your *default group*, unless you change this. In a typical Linux system, your default group
is named like your user and you're the only user belonging to this group.

It's common, however, to create groups to enable collaboration since users and groups are used to
control access to files. You can e.g. create a group for a project and make sure that all files that
should be accessible to members of the project are owned by this group.

When you list a directory in long format, you can see user, group and permissions, together with
many other details:

```
ls -l
-rw-r--r-- 1 dl dl  2526 dec 21 15:54 00-data-organization.Rmd
-rw-r--r-- 1 dl dl 15216 dec 21 16:03 01-unix-and-bash.Rmd
-rw-r--r-- 1 dl dl  4902 dec 21 15:54 02-uppmax.Rmd
-rw-r--r-- 1 dl dl   138 dec  4 10:29 03-regexps.Rmd
-rw-r--r-- 1 dl dl   365 dec 21 15:54 04-automation-with-snakemake.Rmd
-rw-r--r-- 1 dl dl   104 dec  4 10:29 05-notes-on-annotation.Rmd
-rw-r--r-- 1 dl dl   789 dec  6 09:57 06-tidyverse-intro.Rmd
-rw-r--r-- 1 dl dl    65 dec  6 09:57 10-appendices.Rmd
-rw-r--r-- 1 dl dl  2434 dec 21 15:54 11-bash-command-appendix.Rmd
-rw-r--r-- 1 dl dl    56 dec 21 15:54 12-computer-terminology.Rmd
-rw-r--r-- 1 dl dl    91 dec 21 15:54 13-references.Rmd
drwxr-xr-x 4 dl dl  4096 dec 21 15:57 _book
-rw-r--r-- 1 dl dl  2103 dec  6 09:57 eemis-compbio.bib
lrwxrwxrwx 1 dl dl    15 dec 21 15:54 fundamentals_of_computational_biology.pdf -> _book/_main.pdf
drwxr-xr-x 2 dl dl  4096 dec 21 15:54 img
-rw-r--r-- 1 dl dl   907 dec  6 09:57 index.Rmd
-rw-r--r-- 1 dl dl     0 dec  4 09:42 packages.bib
-rw-r--r-- 1 dl dl  1103 dec  6 09:57 README.md
```

The user is the fourth column and the group the fifth.

The first column, looking like `-rw-r--r--` is information about which type of file it is: `-` is a
normal file, `d` a directory and `l` a symbolic link (see \@ref(symlinks)). Next comes permissions
for owner, group and everybody else (the *world*) as triplets: `r` for read, `w` for write and `x`
or `s` for executable. The string `-rw-r--r--` in other words means read and writable for the user
and only readable for anybody else in the group as well as the world. Each part of this string is
called a *bit*.

The third bit, the executable, behaves differently for normal files and directories. For normal
files it means the operating system will treat it like a program and run it if you tell it to. For
directories it means you can `cd` into this directory and use it as your working directory. If
there's an `s` instead of an `x` here it's yet again different between normal files and directories.
For normal files an `s` means that the program will be executed as the owner of the file (`s` in the
first triplet, i.e. as the fourth character) or group (`s` in the second triplet). This is commonly
used for administrative programs that need more permissions than a normal user has. For directories,
you mostly see `s` bit set on the group, which is a way of saying that all files and directories in
this directory will belong to the same group as the directory with the `s` bit set. This is hence an
excellent way of controlling access for a project directory. (In the UPPMAX computers, the `/proj/`
directories are by default owned by the corresponding group and the `s` bit is set.

Ownership, both user and group, is controlled with the `chown` and `chgrp` commands (`chown` can
change both user and group). The following will change ownership of the file `file` to the `dl` user
and the `projects` group:

```
chgrp dl:projects file
```

To manipulate permissions of a file or directory, you use the `chmod` command. The most practical
syntax for this is the *symbolic*, which works like this:

```
# Add writeability to the group
chmod g+w file
# Remove readability from the world
chmod o-r file
# Set the user's permissions to read, write and execute
chmod u=rwx file
```

### File systems {#filesystems}

#### Absolute and relative paths {#paths}

### Links {#links}

A *link* is a name of a file and is created with the `ln` command. Links can be either *hard* or
*symbolic* (*symlinks*) and a file can have many. The "normal" name you see for a file is a hard
link.

Links are created with the `ln` command (with the `-s` flag for symlinks), removed with the `rm`
command and moved, including name changed, with `mv`.

#### Hard links {#hard-links}

Every file must have at least one hard link, but can have many, all of them equivalent. This means
that a file can have many indistinguishable names. They must all reside on the same file system (see
\@ref(filesystems)). A file is not removed until you have removed all links. You can see this
behaviour with the following example:

```
pushd /tmp      # cd to /tmp, but remember where I came from
touch l0        # Create an empty file
for i in {1..9}; do ln l0 l${i}; done   # Create 9 more hard links/names
ls -li l[0-9]   # -l is for long and -i shows the inode number
rm l0           # Remove the first created link
ls -li l[0-9]
rm l[0-9]
ls -li l[0-9]
popd            # cd back to where you were before pushd
```

Note that the first column of the file listing above, the *inode number*, is the same for all,
indicating that they point to the same *inode* and that third column of the first listing has a 10:
the number of links. After the first `rm` the number of links goes down to 9. The file does not
disappear until all links have been removed. In actual fact the *inode* remains until the space is
needed, when it can be removed.

#### Symbolic links -- *symlinks* {#symlinks}

A *symlink* can be thought of as a "shortcut" to a hard link, i.e. a file. Symlinks can reside on
another file system than the hard link it points to and removing one will never remove the file. For
all other purposes, symlinks are equivalent to hard links and can be very useful.

Why using symlinks instead of actual file copies?

1. Unlike file copies symlinks are mere labels, so that a consideable amount of disk memory can be
   saved.

1. They are a safety measure: in case you accidentally overwrite them the original file remains
   unaffected, thus preventing from losing valuable information. Commands which read or write file
   contents will access the contents of the target file. The rm (delete file) and mv (move)
   commands, however, affect the link itself, not the target file.

1. Symbolic links operate transparently: programs accept symlinks as input files, as long as they
   redirect to existing files, and they will behave as if operating directly on the target file.

```
pushd /tmp
touch a_file            # Create an empty file
ln -s a_file a_symlink  # Create a symlink pointing to it
ls -l a_file a_symlink  # List the two
rm a_file a_symlink     # Remove them
popd
```

Note in the row for `a_symlink` in the file listing, that there's an `l` in the first column of the
first field (the permissions) indicating this is a symlink, which is also shown at the end of the
line as `a_symlink -> a_file`.

### Timestamps {#timestamps}

## Editing files {#text-editors}

### Vi/Vim

### Emacs

## Screen {#screen}

## Source control with Git {#git}

## Installing programs with Conda {#conda}

Software installation varies according to the software, this is where package managers come in aid.
*Conda* is a python-based manager that handles software installation and update. Python knowledge is
not required in order to use it. With Conda you can:

1.Readily install a software and all its required dependencies at once.

1.Easily update the installed softwares.

1.store different versions of programs in different environments, because Conda is also an
environment manager.

### How to install Conda {#installing-conda}

*NOTE:* This is a bit long.

*Anaconda* is a science data platform based on the compiling language Python. Namely, *Conda* is
a system for the distribution of Python and also for the collection of more than one thousand
open-source packages therein present.  The quickest way to have Conda is to install *Miniconda*, a
reduced version of Anaconda that includes only Conda and its dependencies.  First, check the System
Requirements [here](https://conda.io/docs/user-guide/install/index.html#system-requirements).  if
one or more of the requirements are missing, you can install these packages by evoking the python
wrapper apt: 

``` 
apt install packagename 
``` 

Once you will have fulfilled all the system requirements you can download and install Miniconda from
[here](https://conda.io/miniconda.html).You need to choose a Miniconda installers suitable for your
operative system.  miniconda executive files contain the conda package manager and a Python version
between 3.6 (miniconda3) and 2.7 (Miniconda).  It is advisable to choose Miniconda3 as most of the
bioinformatics tools in use are compiled in Python 3 and are not correctly interpreted by one of the
versions of Python 2. Regardless, it is possible to have both Pyhton 3 and Python 2 in the same
operative system by creating separate environments with conda, as shown in the next chapter.  For
example, in order to get Miniconda3 for a LINUX environment of 64 bit, type: 

``` 
bash Miniconda3-latest-Linux-x86_64.sh 
``` 

you can know if conda has been correctly installed, and the number of the conda version, by typing: 

``` 
conda --version 
``` 

The following command updates the version of conda in use to the latest version
released: 

``` 
conda update conda 
```

### How to work with Conda {#working-with-conda}

Conda can install and manage the thousand packages and repositories that are collected in the
Anaconda database. 

The all-purpose command for installing programs with conda is:

``` 
conda install -c namechannel namepackage 
```

where the -c flag specifies the channel (e.g. namechannel) that contains said package
(e.g. namepackage). You can look for the channel that contains the package of interest in the [Anaconda Cloud](https://anaconda.org/).
A [tutorial](https://conda.io/docs/user-guide/tutorials/index.html) about how to set Conda and run it has been made available online.
Although conda works both with Python 2.7 and 3.6, only the version 3 is the one 
that is being currently updated. Furtherly, programs like CheckM will support only particular
versions of Python (e.g. 2.7 for CheckM), so sooner or later you might need to create a conda
environment that stores a specific version of a software (e.g. Python).
Suppose we have installed Miniconda3 and are interested in having both Python 2.7 and Python 3.6
available in the same account. The following command line creates a conda environment named pyton2.7
with Python version 2.7 installed in it:

```
conda create --name python2.7 python=2.7
```

An environment (i.e. environmentname) can be activated by typing `source activate environmentname`
Conversely, `source deactivate environmentname` deactivate an environment.  Check the name and
number of stored environments:

```
conda info --envs
```

To see which packages are installed in your current conda environment and their version numbers, run:

```
conda list
```

Note that conda will mark with an asterisk the currently active environment.

A list of the most useful commands can be found in the [Conda cheatsheet](https://conda.io/docs/user-guide/cheatsheet.html).
```{r include=FALSE}
# vim:tw=100
```
