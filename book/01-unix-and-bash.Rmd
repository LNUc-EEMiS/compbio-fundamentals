# Using UNIX-like computers and mastering the command shell {#unix-and-shell}

Most programs in bioinformatics are written for UNIX-like operating systems like Linux. Moreover,
they are usually controlled from the command line (CLI) and not from a graphical interface (GUI).

UNIX and it's derivatives have a long history as an operating system by programmers for programmers.
This makes UNIX different from commercial operating systems and there is a clear philosophy behind
many aspects of behaviour of a computer running UNIX. That being said, a modern UNIX-like operating
system like Linux is in many ways similar to e.g. Apple's OS X or Windows and is fully controllable
via a graphical interface. However, behind the graphical interface the tools many long-time users
have come to love still remain. Apple's OS X is in this way similar to e.g. Linux by being a UNIX at
its core, with a proprietary graphical interface although in this case a commercial, non-free
program. 

## Interacting with UNIX-like operating systems from commercial operating systems {#interacting-with-unix}

In scientific computing, the most common way to connect to a computing cluster running Linux, is to
use Secure Shell (ssh). 

### Connecting from a UNIX-like computer (Linux or Mac OS X) {#connecting-from-unix}

SSH is a command-line tool available in all Linux distributions and in OS X. You connect like this:

```
ssh user@rackham.uppmax.uu.se
```

And your local command-line session is exchanged for one at the cluster.

### Connecting from a Windows computer {#connecting-from-windows}

It's a little bit more complicated with Windows computers, because there's no ssh program directly
available. There are some options though. At UPPMAX they recommend the
[MobaXterm](http://mobaxterm.mobatek.net/) program (see [UPPMAX
documentation](http://www.uppmax.uu.se/support/user-guides/guide--first-login-to-uppmax/)). Another
popular program is [PuTTY](https://putty.org/).

There are also bash implementations for Windows, e.g. [Cygwin](https://www.cygwin.com/) and the
command line interface that comes with [Git for Windows](https://git-scm.com/download/win).

(I also came across a [blog post describing Windows 10's inbuilt Bash
shell](https://www.howtogeek.com/249966/how-to-install-and-use-the-linux-bash-shell-on-windows-10/).)

### Public key authentication {#public-key-authentication}

## Using the Linux command line {#linux-cli}

The Linux command line is a program called the *shell*, and exists in different versions. The most
popular is probably `bash` (short for *B*ourne *a*gain *sh*ell) a derivative of the original *Bourne
shell* `sh`. Together with *Korn shell*, `sh` and `bash` forms a family with similar syntax. A
second family started with *C shell* `csh`, which aimed at being more similar in syntax to the *C*
programming language. In this book, we're going to focus solely on `bash`.

The shell is both a way of interacting interactively with the computer and a programming language, a
*scripting* language. When you use the shell interactively you execute commands and programs and a
shell script is nothing else than a set of commands saved for later, though often generalized with
variables etc. to be more useful.

There are many free sources of information available, one of them the classic [Bash Guide for
Beginners](http://www.tldp.org/LDP/Bash-Beginners-Guide/html/) [@garrels_bash_2008] from "The
Linux Documentation Project" (tldp).

### Getting help {#cli-getting-help}

The classical way of program authors to provide help to users is via manual ("man") pages, readable
with the `man` command. To read the man page for e.g. the find command you:

```
man find
```

The man command uses the `more` command to show the content page by page. Step through pages with
the space key, search with `/` followed by a string and exit by pressing `q`.

Today it's more common to integrate help in the program and make it available with a flag, usually
`--help` or `-v`. In many cases, you get more than one page of output, so you might want to pipe the
output to `more` to read page by page.

```
wget --help|more
```

For complex programs doing many different things it has become increasingly popular to modularise
program invocation and help. A typical example is `git`, which has one overall help plus individual
help for each type of thing you can do with the program.

```
git --help
git clone --help
```

In this particular example, you're actually taken to the manual page for `git clone`, in the latter
of the two commands.

### The environment {#bash-environment}

#### Variables {#bash-variables}

#### Configuration files {#bash-config-files}

### Pipelines {#pipelines}

The word "pipeline" can mean slightly different things in computing. The most original meaning comes
from the UNIX shell, where it means that you literaly connect the output of one command as input to
the next in a chain^[When the word "pipeline" is used in computational biology, it's usually used
symbolicaly and means that in order to produce a certain output you need to run several programs in
succession. A better term for this is *workflow*.]. 
A workflow is in essence a series of independent, repeatable tasks, each with well-defined inputs, parameters, and outputs. This help users perform a complex work by organizing a series of steps, so that files become available for more than one step at a time.

To truly understand what's going on, you need to know about the three different channels that each
UNIX command has: standard input (STDIN), standard output (STDOUT) and standard error (STDERR). By
default, STDIN is connected to your keyboard and the other two to your screen. The difference
between STDOUT and STDERR is that the latter is reserved for error messages from a program, whereas
the former is used for expected output. The channels can be redirected to files which is done with
`<` for STDIN, `>` for STDOUT and `2>` for STDERR. In principle, a command hence looks like this:

```
cat < input_file > output_file 2> error_message_file
```

The above reads `input_file` and sends that to `output_file`; if an error occurs, messages will be
written to `error_message_file`. (The example is a bit constructed, because if you don't redirect
STDIN, `cat` actually reads one or more files on the command line, e.g.: `cat file1 file2 file3 >
concatenated_file`.)

The pipe character `|` connects STDOUT from one command, to STDIN for the next, which makes it
possible to modify the output of one command and form a *pipeline*. A very common example is to
place `more` as the last output of a command that produces a lot of output, so you can read the
output line by line.

```
# Find all files ending with `.tsv` and check output page by page with `more`.
find . -name "*.tsv" | more
```

Pipelines can be arbitrarily long and often contain programs like `grep` (scans for text patterns),
`sed` or `awk` (modifies text), `tail -f` (follow output when it's being produced). This is in line
with the *UNIX philosophy* that programs should do one thing and do it well, leaving other things
for other programs.

### Job control {#job-control}

`&`, `bg`, `fg`, `jobs` and &lt;ctrl&gt;-Z.

## Users, files, directories and file systems {#users-files-and-filesystems}

### Users, groups and permissions {#users-groups-permissions}

### File systems {#filesystems}

The first and most important thing to do in a data analysis is to organize data. It is good practice
to gather all the data into a single folder, henceforth called root directory. The root directory
should be linked to all the other tools that will help in data management, like Screen, Git and Make.

#### Absolute and relative paths {#paths}

### Links {#links}

A *link* is a name of a file and is created with the `ln` command. Links can be either *hard* or
*symbolic* (*symlinks*) and a file can have many. The "normal" name you see for a file is a hard
link.

Links are created with the `ln` command (with the `-s` flag for symlinks), removed with the `rm`
command and moved, including name changed, with `mv`.

#### Hard links {#hard-links}

Every file must have at least one hard link, but can have many, all of them equivalent. This means
that a file can have many indistinguishable names. They must all reside on the same file system (see
\@ref(filesystems)). A file is not removed until you have removed all links. You can see this
behaviour with the following example:

```
pushd /tmp      # cd to /tmp, but remember where I came from
touch l0        # Create an empty file
for i in {1..9}; do ln l0 l${i}; done   # Create 9 more hard links/names
ls -li l[0-9]   # -l is for long and -i shows the inode number
rm l0           # Remove the first created link
ls -li l[0-9]
rm l[0-9]
ls -li l[0-9]
popd            # cd back to where you were before pushd
```

Note that the first column of the file listing above, the *inode number*, is the same for all,
indicating that they point to the same *inode* and that third column of the first listing has a 10:
the number of links. After the first `rm` the number of links goes down to 9. The file does not
disappear until all links have been removed. In actual fact the *inode* remains until the space is
needed, when it can be removed.

#### Symbolic links -- *symlinks* {#symlinks}

A *symlink* is a file that contains a text string that is automatically interpreted by
the operating system as a path to another file or directory. Simply put,it can be thought of as a "shortcut" to a hard link. Symlinks can reside on another file
system than the hard link it points to and removing one will never remove the file. For all other
purposes, symlinks are equivalent to hard links and can be very useful.
Why using symlinks instead of actual file copies?
1. Unlike file copies symlinks are mere labels, so that a consideable amount of disk memory can be saved.
1. They are a safety measure: in case you accidentally overwrite them the original file remains
unaffected, thus preventing from losing valuable information. Commands which read or write file
contents will access the contents of the target file. The rm (delete file) and mv (move) commands,
however, affect the link itself, not the target file.
1. Symbolic links operate transparently: programs accept symlinks as input files, as long as they
redirect to existing files, and they will behave as if operating directly on the target file.

```
pushd /tmp
touch a_file            # Create an empty file
ln -s a_file a_symlink  # Create a symlink pointing to it
ls -l a_file a_symlink  # List the two
rm a_file a_symlink     # Remove them
popd
```

Note in the row for `a_symlink` in the file listing, that there's an `l` in the first column of the
first field (the permissions) indicating this is a symlink, which is also shown at the end of the
line as `a_symlink -> a_file`.

### Timestamps {#timestamps}

## Screen {#screen}

## Source control with Git {#git}

## Installing programs with Conda {#conda}

Software installation varies according to the software, this is where package managers come in aid.
**Conda** is a python-based manager that handles software installation and update. Python
knowledge is not required in order to use it. With Conda you can:

1.Readily upload a software and all its required dependencies at once.
1.Easily update the installed softwares.
1.store different versions of programs in different environments, because Conda is also an environment manager.

### How to install Conda {#installing-conda}

**Anaconda** is a science data platform based on the compiling language Python. Namely, **Conda** is a system for the distribution of Python and also for the collection of more than one thousand open-source packages therein present.
The quickest way to have Conda is to install **Miniconda**, a reduced version of Anaconda that includes only Conda and its dependencies.
First, check the System Requirements [here](https://conda.io/docs/user-guide/install/index.html#system-requirements).
if one or more of the requirements are missing, you can install these packages by evoking the python wrapper apt:
```
apt install packagename
```
Once you will have fulfilled all the system requirements you can download and install Miniconda
from [here](https://conda.io/miniconda.html).You need to choose a Miniconda installers suitable for your operative system.
miniconda executive files contain the conda package manager and a Python version between 3.6
(miniconda3) and 2.7 (Miniconda). It is advisable to choose Miniconda3 as most of the bioinformatics
tools in use are compiled in Python 3 and are not correctly interpreted by one of the versions of
Python 2. Regardless, it is possible to have both Pyhton 3 and Python 2 in the same operative system
by creating separate environments with conda, as shown in the next chapter.
For example, in order to get Miniconda3 for a LINUX environment of 64 bit, type:
```
bash Miniconda3-latest-Linux-x86_64.sh
```
you can know if conda has been correctly installed, and the number of the conda version, by typing:
```
conda --version
```
The following command updates the version of conda in use to the latest version released:
```
conda update conda
```

### How to work with Conda {#working-with-conda}

Conda can install and manage the thousand packages and repositories that are collected in the
Anaconda database. 
The all-purpose command for installing programs with conda is:
``` conda install -c namechannel namepackage ```
where the -c flag specifies the channel (e.g. namechannel) that contains said package
(e.g. namepackage). You can look for the channel that contains the package of interest in the [Anaconda Cloud](https://anaconda.org/).
A [tutorial](https://conda.io/docs/user-guide/tutorials/index.html) about how to set Conda and run it has been made available online.
Although conda works both with Python 2.7 and 3.6, only the version 3 is the one 
that is being currently updated. Furtherly, programs like CheckM will support only particular
versions of Python (e.g. 2.7 for CheckM), so sooner or later you might need to create a conda
environment that stores a specific version of a software (e.g. Python).
Suppose we have installed Miniconda3 and are interested in having both Python 2.7 and Python 3.6
available in the same account. The following command line creates a conda environment named pyton2.7
with Python version 2.7 installed in it:
```conda create --name python2.7 python=2.7```
An environment (i.e. environmentname) can be activated by typing ```source activate environmentname```
Conversely, ```source deactivate environmentname``` deactivate an environment
Check the name and number of stored environments:

```
conda info --envs
```
To see which packages are installed in your current conda environment and their version numbers, run:
```
conda list
```
Note that conda will mark with an asterisk the currently active environment.

A list of the most useful commands can be found in the [Conda cheatsheet](https://conda.io/docs/user-guide/cheatsheet.html).
```{r include=FALSE}
# vim:tw=100
```
