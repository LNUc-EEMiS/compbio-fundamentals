---
title: "Centered log ratio analysis of 16S data"
author: "daniel.lundin@lnu.se"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
  pdf_document:
    fig_caption: yes
    fig_height: 9
    fig_width: 8
    number_sections: yes
    toc: yes
  html_document:
    toc: yes
---

```{r setup, echo=F, cache = FALSE}
knitr::opts_chunk$set(echo=F, fig.path='figures/', cache = TRUE)
ggplot2::theme_set(ggplot2::theme_bw())
```

```{r libraries, message=F, cache = FALSE}
suppressPackageStartupMessages(library(readr))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(tidyr))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(kfigr))
suppressPackageStartupMessages(library(knitr))

# Packages needed to calculate clr
library(zCompositions)
library(CoDaSeq)
```

```{r constants}
```

```{r read-data}
# Read the ASV table and turn it long without zeroes
asvs <- read_tsv(
  '../../data/atacama-soils.asvtable.tsv',
  col_types = cols(.default = col_double(), seqid = col_character())
) %>%
  gather(sample, count, 2:67) %>%
  # Take away rows with zero count
  filter(count > 0) %>%
  # Take away samples with less than 500 observations
  group_by(sample) %>% filter(sum(count) >= 500) %>% ungroup()

# Read the taxonomy table
taxonomy <- read_tsv(
  '../../data/taxonomy.tsv',
  col_types = cols(
    .default = col_character(), Confidence = col_double()
  )
) %>%
  # Delete the D_0__ etc strings at the start of each taxon
  mutate(
    Taxon = gsub('D_[0-9]_+', '', Taxon)
  ) %>%
  # Rename Feature ID to seqid, the name in the ASV table
  rename(
    seqid = `Feature ID`,
  ) %>%
  # Separate th single taxonomy string into indvidiual taxa
  separate(
    Taxon, 
    c('domain', 'phylum', 'class', 'order', 'family', 'genus', 'species'), 
    sep = ';',
    fill = 'right'
  )

# Read the sample data ("metadata")
samples <- read_tsv(
  '../../data/sample_metadata.tsv',
  col_types = cols(
    .default = col_double(),
    SampleID = col_character(), BarcodeSequence = col_character(), LinkerPrimerSequence = col_character(),
    ExtractGroupNo = col_character(), TransectName = col_character(), SiteName = col_character(),
    Vegetation = col_character(), Description = col_character()
  )
)
```

# Centered Log Ratios

*C*entered *L*og *R*atios (clrs) are calculated as log of the ratio between a count and the geometric mean of counts in 
the sample:

$$
clr_s = \log{\frac{c_s}{\sqrt[n]{\prod_{i = 1}^{n} c_i}}}
$$

Where $n$ is the number of samples, $cls_s$ is the centered log ratio and $c_s$ the count respectively for sample $s$.

## Dealing with zeroes

The scary thing in the denominator, the geometric mean of the sample, i.e. root of the product of all counts in the sample,
will be zero if there's a single  zero, i.e. unobserved taxon (ASV), in a sample. The root
will then be zero and since division by zero is undefined, the clr will be too. Since a typical biological sequencing data
set always contains a lot of zeroes, clrs will typically be undefined for all samples. To use clr, the zeroes therefore
need to be removed or replaced with something else.

The zeroes are artefacts of sequencing depth, and *not evidence for the absence of a population* in a sample. Methods that
deal with zeroes can hence be based on estimation of the true count (<1) of a population or similar. The `cmultRepl()`
function in the `zCompositions` package can help us replace the zeroes with very small *pseudocounts* ("p-counts").

*** > Read the Palarea-Albaladejo 2013 paper! <

Terms:

* *left-censored*: a data point is below a certain value. In our case this means below detection limit given the sequencing
depth for the sample in question.

```{r calc-clr}
# This adds a clr column to the asvs table.
# Note that the assignment to "asvs" is last and uses the -> assignment operator.
# Since all combinations of sample and seqid will now get a value, clr, the number of rows increases quite a lot.
asvs %>%
  # Make the table wide with samples as columns
  spread(sample, count, fill = 0) %>%
  # Move the seqid to rowname; this requires a data.frame()
  data.frame() %>% tibble::column_to_rownames('seqid') %>%
  # Replace zeroes with probabilities (pseudocounts) (I needed a slightly lower delta than default not to get negative values)
  cmultRepl(method = 'CZM', delta = 0.5, output = 'p-counts') %>%
  # Calculate the CLR
  codaSeq.clr(samples.by.row = FALSE) %>%
  data.frame() %>%
  tibble::rownames_to_column('sample') %>%
  gather(seqid, clr, 2:ncol(.)) %>%
  # Get rid of 'X' that sometimes precedes the seqid and join back with original table
  mutate(seqid = sub('^X', '', seqid)) %>%
  left_join(asvs, by = c('sample', 'seqid')) %>%
  # Set count to 0 if it's NA
  replace_na(list(count = 0)) -> asvs
```

## Some characteristics of clrs

Centered log ratios do not sum to the same sum for all samples -- in fact, they sum to many orders of magnitude different
values `r figr('clrsum-plot', T, type = 'Figure')` -- and they are often
negative (beacuse one takes the log). They are hence not good replacements for relative abundances in e.g. heatmaps.

```{r clrsum-plot, fig.height=6, fig.cap = 'Per sample sums of centered log ratios.'}
asvs %>%
  group_by(sample) %>% summarise(clrsum = sum(clr)) %>% ungroup() %>%
  ggplot(aes(x = sample, y = clrsum)) +
  geom_point() +
  coord_flip()
```

# Redundancy analysis

Redundancy analysis (RDA) is an ordination method that can be constrained by sample metadata or not. In the latter case, it is
identical to principle component analysis (PCA). The fact that we with clrs have data that is behaving nicely in statistical
terms, allowing e.g. euclidian distances to be measured, makes it possible for us to us this very powerful methodology.

We start by looking at how PCA works, i.e. only studying the distribution of samples and taxa, using the Vegan package's
`rda` function.

```{r calc-pca}
asvs %>% 
  # Standard Vegan transformation: Turn table with with samples as *rows*
  dplyr::select(sample, seqid, clr) %>%
  spread(seqid, clr) %>%
  # Turn into a numeric matrix
  tibble::column_to_rownames('sample') %>% as.matrix() %>%
  # And call Vegan's rda function that will just do pca unless you give it a 
  # a second argument (constraining matrix)
  vegan::rda() -> pca
```

What's returned by the `rda` funtion is a list object that can be plotted with the base `plot` function, but we can also
pick out the necessary parts to make a PCA *biplot* of samples and ASVs.

```{r pca-biplot}
pca.samples <- pca$CA$u %>% data.frame() %>% tibble::rownames_to_column('sample')
pca.asvs    <- pca$CA$v %>% data.frame() %>% tibble::rownames_to_column('asv')

# We use the pca.samples table as the "main" table when calling ggplot.
# Let's first join it with the samples table so we can use some metadata
# for colouring.
pca.samples %>%
  inner_join(samples, by = c('sample' = 'SampleID')) %>%
  ggplot(aes(x = PC1, y = PC2)) +
    # Plot the ASVs *behind* the samples, i.e. first
    geom_point(data = pca.asvs, size = 0.1) +
    # Points for samples, coloured by humidity
    geom_point(aes(colour = AverageSoilRelativeHumidity)) +
    scale_colour_viridis_c('Average humidity')
```

