---
title: "Centered log ratio analysis of 16S data"
author: "daniel.lundin@lnu.se"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
  pdf_document:
    fig_caption: yes
    fig_height: 9
    fig_width: 8
    number_sections: yes
    toc: yes
  html_document:
    toc: yes
bibliography: [../../book/eemis-compbio.bib]
---

```{r setup, echo=F, cache = FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.path = 'figures/', cache = TRUE)
ggplot2::theme_set(ggplot2::theme_bw())
```

# Background: Centered Log Ratios

Recently, the best practices for analyses of count data -- e.g. sequencing data from 16S amplicons,
metagenomes or metatranscriptomes -- has been debated [@mcmurdie_waste_2014]. The fact that
sequencing depth, i.e. the number of sequences per sample, is not related to the size of the sampled
community but largely an effect of random processes suggests that the absence of a sequence, ASV or
taxon is a sample is not evidence for the true absence of this population. Instead, the population
was below the detection limit of the sequencing of this particular sample. The proper type of
statistical tools for this kind of data -- count data with a detection limit -- are called
"compositional". 

One of the suggested approaches for dealing with compositional data is to convert counts to
*C*entered *L*og *R*atios (CLRs) [@gloor_microbiome_2017]. CLRs have a normal distribution,
suggesting we can use them in many statistical tests and procedures that requires normally
distributed data, like principal component analysis (PCA). With the older approaches -- relative
abundances or subsampled ("rarefied") data -- we had to resort to methods that were more robust to
non-normally distributed data but in many cases not as powerful. One particular aspect of CLRs is
that it's correct to calculate *euclidian distances* from them, a requirement for instance in PCA.
The euclidian distance replaces other measurements of $beta$-diversity like Bray-Curtis or Unifrac.

CLRs emphasize *variance* and not abundance, so that samples that differ by the rare, variable part
of the community will separate better than those differing by smaller amounts of the more abundant
populations. If you want to focus on the influence of the most abundant populations instead of the
most variable, there are other measurements to use, e.g. the Hellinger distance.

CLRs are useful when you work with comparisons between samples or taxa, or with correlation
analyses, but they are *not applicable to calculations of $alpha$-diversity* nor in visualizations
of taxon abundances. Some work has been done in utilizing the power of the collected data also for
$alpha$-diversity [e.g.  @willis_rarefaction_2017], but I will not go into that here.

## Calculation of centered log ratios

CLRs are calculated as the logarithm of the ratio between a count and the geometric mean of counts
in the sample:

$$
clr_s = \log{\frac{c_s}{\sqrt[n]{\prod_{i = 1}^{n} c_i}}}
$$

Where $n$ is the number of samples, $cls_s$ the centered log ratio and $c_s$ the count
respectively for sample $s$.

## The Aitchison distance

If one takes the euclidian distance between samples based on the CLR, one gets what is also called
the "Aitchison distance", see e.g. [@gloor_microbiome_2017].

## Dealing with zeroes

The scary thing in the denominator, the geometric mean of the sample, i.e. root of the product of
all counts in the sample, will be zero if there's a single  zero, i.e. unobserved taxon (ASV), in a
sample. The root will then be zero and since division by zero is undefined, the clr will be too.
Since a typical biological sequencing data set always contains a lot of zeroes, clrs will typically
be undefined for all samples. To use clr, the zeroes therefore need to be removed or replaced with
something else.

The zeroes are artefacts of sequencing depth, and *not evidence for the absence of a population* in
a sample. Methods that deal with zeroes can hence be based on estimation of the true count (<1) of a
population or similar. The `cmultRepl()` function in the `zCompositions` package can help us replace
the zeroes with very small *pseudocounts* ("p-counts").

# Necessary libraries

Besides the Tidyverse libraries, you will need
[zCompositions](https://cran.r-project.org/web/packages/zCompositions/index.html) and
[CoDaSeq](https://github.com/ggloor/CoDaSeq). The former is available on CRAN and hence easy to
install, while the latter is only available through GitHub, which means it should be installed using
`devtools::install_github()`. It also requires
[ALDEx2](https://bioconductor.org/packages/release/bioc/html/ALDEx2.html) which is a Bioconductor
package.

## Installation

```
# Install zCompositions from CRAN
install.packages('zCompositions')

# ALDEx2 from Bioconductor
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install("ALDEx2")

# and CoDaSeq from GitHub
install.packages('devtools')
devtools::install_github('ggloor/CoDaSeq/CoDaSeq')
```

# Workflow

## Load required libraries

```{r libraries, message=F, cache = FALSE}
suppressPackageStartupMessages(library(readr))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(tidyr))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(kfigr))
suppressPackageStartupMessages(library(knitr))

# Packages needed to calculate clr
library(zCompositions)
library(CoDaSeq)
```

## Read data files

We start by reading in the data files: ASVs (counts), sample data ("metadata") and the taxonomy.

```{r read-data}
# Read the ASV table and turn it long without zeroes
asvs <- read_tsv(
  '../../data/atacama-soils.asvtable.tsv',
  col_types = cols(.default = col_double(), seqid = col_character())
) %>%
  gather(sample, count, 2:67) %>%
  # Take away rows with zero count
  filter(count > 0) %>%
  # Take away samples with less than 500 observations (this is an arbitrary choice 
  # to get rid of samples with very low sequencing depth).
  group_by(sample) %>% filter(sum(count) >= 500) %>% ungroup()

# Read the taxonomy table
taxonomy <- read_tsv(
  '../../data/taxonomy.tsv',
  col_types = cols(
    .default = col_character(), Confidence = col_double()
  ) 
) %>%
  # Delete the D_0__ etc strings at the start of each taxon
  mutate(
    Taxon = gsub('D_[0-9]_+', '', Taxon)
  ) %>%
  # Rename Feature ID to seqid, the name in the ASV table
  rename(
    seqid = `Feature ID`,
  ) %>%
  # Separate th single taxonomy string into indvidiual taxa
  separate(
    Taxon, 
    c('domain', 'phylum', 'class', 'order', 'family', 'genus', 'species'), 
    sep = ';',
    fill = 'right'
  )

# Read the sample data ("metadata")
samples <- read_tsv(
  '../../data/sample_metadata.tsv',
  col_types = cols(
    .default = col_double(),
    SampleID = col_character(), BarcodeSequence = col_character(), 
    LinkerPrimerSequence = col_character(), ExtractGroupNo = col_character(), 
    TransectName = col_character(), SiteName = col_character(),
    Vegetation = col_character(), Description = col_character()
  )
) %>%
  rename(sample = SampleID) %>%
  # Subset the table to only contain the samples that are in the asvs table (which was 
  # subset to only contain samples > 500 observations).
  semi_join(asvs, by = 'sample')
```

## Calculate CLR

We want to have a new column called `clr` in the asvs table.

```{r calc-clr}
# This *adds a clr column* to the asvs table.
# Note that the assignment to "asvs" is last and uses the -> assignment operator.  
# Since all combinations of sample and seqid will now get a value, clr, the number of rows
# increases quite a lot.
asvs %>%
  # Make the table wide with samples as columns
  spread(sample, count, fill = 0) %>%
  # Move the seqid to rowname; this requires a data.frame()
  data.frame() %>% tibble::column_to_rownames('seqid') %>%
  # Replace zeroes with probabilities (pseudocounts) (I needed a slightly lower delta than default
  # not to get negative values)
  cmultRepl(method = 'CZM', delta = 0.5, output = 'p-counts') %>%
  # Calculate the CLR
  codaSeq.clr(samples.by.row = FALSE) %>%
  data.frame() %>%
  tibble::rownames_to_column('sample') %>%
  gather(seqid, clr, 2:ncol(.)) %>%
  # Get rid of 'X' that sometimes precedes the seqid and join back with original table
  mutate(seqid = sub('^X', '', seqid)) %>%
  left_join(asvs, by = c('sample', 'seqid')) %>%
  # Set count to 0 if it's NA
  replace_na(list(count = 0)) -> asvs
```

## Redundancy analysis

Redundancy analysis (RDA) is an ordination method that can be constrained by sample metadata or not.
In the latter case, it is identical to principle component analysis (PCA). The fact that we with
clrs have data that is behaving nicely in statistical terms, allowing e.g. euclidian distances to be
measured, makes it possible for us to us this very powerful methodology.

We start by looking at how PCA works, i.e. only studying the distribution of samples and taxa, using
the Vegan package's `rda` function.

```{r calc-pca}
asvs %>% 
  # Standard Vegan transformation: Turn table with with samples as *rows*
  dplyr::select(sample, seqid, clr) %>%
  spread(seqid, clr) %>%
  # Turn into a numeric matrix
  tibble::column_to_rownames('sample') %>% as.matrix() %>%
  # And call Vegan's rda function that will just do pca unless you give it a 
  # a second argument (constraining matrix)
  vegan::rda() -> pca
```

What's returned by the `rda` funtion is a list object that can be plotted with the base `plot`
function, but we can also pick out the necessary parts to make a PCA *biplot* of samples and ASVs
using `ggplot2`.

```{r pca-biplot, fig.height = 6, fig.cap = 'PCA of the samples. Coloured circles are the samples, and black dots the ASVs positions in the PCA coordinate system.'}
pca.samples <- pca$CA$u %>% data.frame() %>% tibble::rownames_to_column('sample')
pca.asvs    <- pca$CA$v %>% data.frame() %>% tibble::rownames_to_column('asv')
pca.eigs    <- pca$CA$eig %>% data.frame() %>% tibble::rownames_to_column('pc') %>%
  rename(eigval = 2) %>%
  mutate(propexpl = eigval/sum(eigval))

# We use the pca.samples table as the "main" table when calling ggplot.
# Let's first join it with the samples table so we can use some metadata
# for colouring.
pca.samples %>%
  inner_join(samples, by = 'sample') %>%
  ggplot(aes(x = PC1, y = PC2)) +
    # Plot the ASVs *behind* the samples, i.e. first
    geom_point(data = pca.asvs, size = 0.1) +
    # Points for samples, coloured by humidity
    geom_point(aes(colour = AverageSoilRelativeHumidity)) +
    scale_colour_viridis_c('Average humidity') +
    xlab(sprintf("PC1 (%2.1f%% explained)", pca.eigs[1,3] * 100)) +
    ylab(sprintf("PC2 (%2.1f%% explained)", pca.eigs[2,3] * 100)) 
```

## Proper RDA with sample data

Below, I'm selecting three variables from the metadata, average soil relative humidity, average soil
temperature (both continuous) and vegetation (categorical, i.e. yes or no).

```{r calc-rda-2vars}
# Create a matrix object; we need it named, can't generate one on "the fly"
asvs %>% dplyr::select(sample, seqid, clr) %>%
  spread(seqid, clr) %>% tibble::column_to_rownames('sample') -> asv_matrix

# Here's the call to the rda function with a formula as the first argument,
vegan::rda(
  asv_matrix ~ AverageSoilRelativeHumidity + AverageSoilTemperature + Vegetation,
  data = samples %>%
    semi_join(asvs, by = 'sample') %>%
    replace_na(list('AverageSoilRelativeHumidity' = 0.5, 'AverageSoilTemperature' = 20)) %>%
    tibble::column_to_rownames('sample')
) -> rda
```

The results of an RDA analysis can be plotted as a *triplot* with arrows pointing in the direction
of the environmental parameters (`r figr('rda-triplot', T, type = 'Figure')`). (Usually categorical
variables are plotted as points and not as arrows as in my figure.)

```{r rda-triplot, fig.height = 6, fig.cap = 'Redundancy analysis (RDA). Samples are plotted together with vectors indicating the influence of the three factors included in the analysis. *Note 1*: Many samples got the same coordinates, and are hence plotted on top of each other. *Note 2*: that the lengths of the factor vectors were divided by four to fit inside the plot. They should hence only be interpreted as directions.'}
# I need to collect some temporary data sets for the plot

# This one will contain the proportions explained which we get by dividing the
# eigenvalue of each component with the sum of eigenvalues for all components.
p <- c(rda$CCA$eig, rda$CA$eig)/sum(c(rda$CCA$eig, rda$CA$eig))

# This one is collecting the coordinates for arrows that will depict how the 
# factors in our model point in the coordinate
a <- rda$CCA$biplot %>% data.frame() %>% tibble::rownames_to_column('factor')

rda$CCA$u %>% data.frame() %>% tibble::rownames_to_column('sample') %>%
  inner_join(samples, by = 'sample') %>%
  ggplot(aes(x = RDA1, y = RDA2)) +
    geom_point(aes(colour = AverageSoilRelativeHumidity)) +
    scale_colour_viridis_c('Rel. humidity') +
    xlab(sprintf("RDA1 (%2.1f%% explained)", p[[1]] * 100)) +
    ylab(sprintf("RDA2 (%2.1f%% explained)", p[[2]] * 100)) +
    # I'm dividing the RDA1 and RDA2 values by four, since they are too long in
    # comparison with the coordinates for the data points (perhaps not advisable
    # in real life...)
    geom_segment(
      data = a, mapping = aes(xend = RDA1/4, yend = RDA2/4), 
      x = 0, y = 0, arrow = arrow()
    ) +
    # Print the names of factors halfway along the arrows
    geom_text(
      data = a, mapping = aes(x = RDA1/8, y = RDA2/8, label = factor), 
      size = 4
    )
```

# References

